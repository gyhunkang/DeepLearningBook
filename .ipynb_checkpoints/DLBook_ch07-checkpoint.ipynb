{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization for Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Regularization</strong> - any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most regularization strategies are based on <strong><em>regularizing estimators</em></strong><br>\n",
    "Regularization of an estimator works by trading <strong> increased bias </strong> for <strong> reduced variance </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three situations:\n",
    "1. excluded the true data generating process - corresponding to underfitting and inducing bias\n",
    "1. matched the true data generating process\n",
    "1. included the generating process buy also many other possible generating processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Goal of Regularization is to take a model from the <strong><em> third regime </em></strong> into the <strong><em>second regime</em></strong> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, We almost never have access to the true data generating process so\n",
    "we can never know for sure if the model family being estimated includes the\n",
    "generating process or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " controlling the complexity of the model is not a\n",
    "simple matter of finding the model of the right size, with the right number of\n",
    "parameters <br>\n",
    "Instead, we might find that the <strong><em>best fitting model </em></strong>(in the sense of minimizing generalization error) is a <strong><em>large model that has been regularized appropriately.</em></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Parameter Norm Penalties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many regularization approaches are based on limiting the capacity of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note that for neural networks, we typically choose to use a parameter norm penalty that â„¦ penalizes of the affine transformation at each layer and leaves only the weights the biases unregularized. Each bias controls only a single variable. This means that we do not induce too much variance by leaving the biases unregularized.  Also, regularizing the bias parameters can introduce a significant amount of underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is still reasonable to use the same weight decay at all layers just to reduce the size of search space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.1 $\\;L^2$ Parameter Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a.k.a. <strong> weight decay, ridge regression, Tikhonov regularization </strong> <br>\n",
    "This strategy drives the weights closer to the origin by adding a regularization term ${\\Omega}({\\Theta})\\; = \\; \\frac{1}{2}\\left\\Vert{\\omega}\\right\\Vert^2_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\omega \\; = \\; Q(A\\;+\\alpha I)^{-1}AQ^T\\omega^* $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "effect of weight decay is to rescale $\\omega$ along the axes defined by the eigenvectors of $H\\;=\\;QAQ^T $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, the component of $\\omega$ that is alogned with the $i$-th eigenvector of $H$ is rescaled by a factor of $\\frac{\\lambda_i}{\\lambda_i + \\alpha}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along the direction where eigenvalues of $H$ are relatively large (ie. $\\lambda_i >> \\alpha$ ), effect of regularization is relatively small. <br>\n",
    "However, components with $\\lambda_i << \\alpha $ will be shrunk to have nearly zero magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the directions along which the parameters contribute significantly to reducing the objective function are preserved relatively intact. <br>\n",
    "In directions that do <strong>NOT</strong> contribute to reducing the objective function, a <strong>small eigenvalue of the Hessian</strong> tells us that movement in this direction will <strong>not</strong> significantly increase the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.2 $\\;L^1$ Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Omega (\\theta) \\; = \\;  \\left\\Vert{\\omega}\\right\\Vert_1 \\; = \\sum_{i} \\left\\vert {\\omega_i}\\right\\vert$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison to $L^2$ regularization, $L^1$ regularization results in a solution that is more <strong>sparse</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparsity property induced by $L^1$ regularization has been used extensively as a <strong>feature selection</strong> mechanism <br>\n",
    "<strong> LASSO </strong> (Least Absolute Shrinkage and Selection Operator) model integrates an $L^1$ penalty with a linear model and a least squares cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L^2$ ~ MAP Bayesian inference with a Guassian prior on the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L^1$ ~ MAP Bayesian inference with an isotropic Laplace distribution on the weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
