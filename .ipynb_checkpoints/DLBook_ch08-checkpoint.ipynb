{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization for Training Deep Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 How Learning Differes from Pure Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>ML/DL Optimization </strong>: <br>\n",
    " 1. acts indirectly - reduce cost function $J(\\theta)$ to improve $P$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Pure Optimization</strong>: <br>\n",
    "1. acts directly - minimize $J$ is the goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective function: <br>\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "J^{*}(\\theta) =  \\mathop{\\mathbb{E}}_{(x,y) \\sim p_{data}} L(f(x;\\theta),y)\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.1 Empirical Risk Minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above equation is known as <strong> risk </strong>\n",
    "<br> but in reality, the true underlying distribution $p_{data}$ is not known\n",
    "<br> replacing the true distribution $p(x,y)$ with empirical distribution $\\hat{p}(x,y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> empirical risk: </strong>\n",
    "\\begin{equation*}\n",
    "\\mathop{\\mathbb{E}}_{x,y \\sim \\hat{p}_{data}(x,y)} [L(f(x;\\theta),y)] = \\frac{1}{m} \\sum\\limits_{i=1}^m L(f(x^{(i)};\\theta), y^{(i)})\n",
    "\\end{equation*}\n",
    "where $m$ is the number of training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, empirical risk minimization is prone to overfitting <br>\n",
    " - models with High Capacity can simply memorize the training set\n",
    " - many useful loss functions such as 0-1 loss have no useful derivatives\n",
    "<br>$\\therefore$ emprical risk minimization is rarely used in DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.2 Surrogate Loss Functions and Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> surrogate loss function </strong> is used as a proxy, and has advantages over the original loss function. \n",
    "<br> Negative Log-Likelihood is typically used as a surrogate for the 0-1 loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important Difference between Optimization in General and Optimization in ML/DL:\n",
    "<br> ML usually minimizes a surrogate loss function but <strong> halts when a convergence criterion based on early stopping is satisfied </strong> \n",
    "<br>whereas for the pure optimization, it is considered to have converged when the <strong> gradient becomes very small </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.3 Batch and Minibatch Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML: Objective function usually decomposes as a <strong> sum over the training examples </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Example, MLE: <br>\n",
    "\n",
    "\\begin{equation*}\n",
    "\\theta_{ML} = arg\\max\\limits_{\\theta} \\sum\\limits_{i=1}^m log p_{model} (x^{(i)}, y^{(i)}; \\theta) \n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximizing this sum is equivalent to maximizing the Expectation over the Empirical distribution defined by the training set: <br>\n",
    "\\begin{equation*}\n",
    "\\mathop{\\mathbb{E}}_{x,y \\sim \\hat{p}_{data}} log p_{model}(x, y; \\theta)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most commonly used property is the gradient: <br>\n",
    "\\begin{equation*}\n",
    "\\nabla_{\\theta} \\:J(\\theta) = \\mathop{\\mathbb{E}}_{x,y \\sim \\hat{p}_{data}} \\nabla_{\\theta}\\: log\\: p_{model} (x, y; \\theta)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing this expectation exactly is very expensive <br>\n",
    " - instead, compute these expectations by randomly sampling a small number of examples from the dataset, then taking the Average over only those examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The Standard Error of the Mean is given by: <br>\n",
    "\\begin{equation*}\n",
    "SE(\\hat{\\mu}_m) = \\sqrt{Var\\bigg[\\frac{1}{m}\\sum\\limits_{i=1}^mx^{(i)}\\bigg]} = \\frac{\\sigma}{\\sqrt{m}}\n",
    "\\end{equation*}\n",
    "<br> where $\\sigma$ is the true standard deviation of the value of the samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The denominator of $\\sqrt{m}$ shows that there iare <strong> less than linear returns </strong> to using more examples to estimate the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We may find large number of examples that all make very similar contributions to the gradient - <strong>redundant samples</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> batch / deterministic </strong>: use entire training set <br>\n",
    "<strong> online </strong>: use examples drawn from a continuous stream of new data <br>\n",
    "<strong> minibatch / stochastic </strong>: use more than one but less than all of training examples <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "minibatch sizes are decided by following factors:\n",
    "- larger batches = more accurate estimate of gradient, but with less than linear returns\n",
    "- small batches do not utilize multi-core architectures\n",
    "- small batches can offer a regularizing effect - due to noise they add to learning process. Generalization error is often best for a batch size of 1. But learning will take more time - more steps + reduced learning rate to maintain stability due to high variance in gradient estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update methods based only on the gradient $\\mathbf{g}$ are usually relatively robust and can handle smaller batch sizes like 100. <br>\n",
    "Second-order methods that compute updates such as $\\mathbf{H^{-1}g}$ typically require much larger batch sizes like 10,000. - to minimize fluctuations in the estimates of *$\\mathbf{H^{-1}g}$*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\mathbf H$ is poorly conditioned, very small changes in the estimate of $\\mathbf g$ can cause large changes in the update $\\mathbf H^{-1}g$, even if $\\mathbf H$ were estimated perfectly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from **4.2 Poor Conditioning**: <br>\n",
    "$f(x) \\; = \\; A^{-1}x $ <br>\n",
    "$A \\; \\in \\; \\mathbb{R}^{n\\times n} $ has an eigenvalue decomposition<br>\n",
    "condition number is: $\\max \\limits_{i,j}\\|\\frac{\\lambda_i}{\\lambda_j}\\|$\n",
    "<br> This is the ratio of the magnitude of the largest and smallest eigenvalue\n",
    "<br> when condition number is large, matrix inversion is particularly sensitive to error in the input\n",
    "<br> Poorly conditioned matrices amplify pre-existing erros when we multiply by the true matrix inverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important to select minibatches **randomly**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
